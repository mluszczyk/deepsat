{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_and_concat(sequences):  # sequences shape: [batch_size, len, dims...] -> ([batch_size, maxlen, dims...], [len])\n",
    "    arrays = [np.asarray(seq) for seq in sequences]\n",
    "    lengths = np.asarray([array.shape[0] for array in arrays], dtype=np.int32)\n",
    "    maxlen = np.max(lengths)\n",
    "    arrays = [np.pad(array, [(0, maxlen - array.shape[0]), (0, 0)], 'constant', constant_values=0) for array in arrays]\n",
    "    return np.asarray(arrays), lengths\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE = 200\n",
    "VARIABLE_NUM = 4\n",
    "EMBEDDING_SIZE = 8\n",
    "CLAUSE_SIZE = 3\n",
    "CLAUSE_NUM = 30\n",
    "LSTM_STATE_SIZE = 8\n",
    "batch_size = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assert_shape(matrix, shape: list):\n",
    "    act_shape = matrix.get_shape().as_list()\n",
    "    assert act_shape == shape, \"got shape {}, expected {}\".format(act_shape, shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class Graph:\n",
    "    def __init__(self):\n",
    "        self.inputs = tf.placeholder(tf.int32, shape=(batch_size, None, CLAUSE_SIZE), name='inputs')\n",
    "        self.lengths = tf.placeholder(tf.int32, shape=(batch_size,), name='lengths')\n",
    "        self.policy_labels = tf.placeholder(tf.float32, shape=(batch_size, VARIABLE_NUM*2), name='labels')\n",
    "        \n",
    "        vars_ = tf.abs(self.inputs)\n",
    "        signs = tf.cast(tf.sign(self.inputs), tf.float32)  # shape: [batch_size, None, CLAUSE_SIZE]\n",
    "\n",
    "        embeddings = tf.Variable(tf.random_uniform([VARIABLE_NUM + 1, EMBEDDING_SIZE], -1., 1), name='embeddings')\n",
    "\n",
    "        var_embeddings = tf.nn.embedding_lookup(embeddings, vars_)\n",
    "        # var_embeddings shape: [None, None, CLAUSE_SIZE, EMBEDDING_SIZE]\n",
    "        \n",
    "        clause_preembeddings = tf.concat(\n",
    "            [tf.reshape(var_embeddings, [batch_size, -1, CLAUSE_SIZE * EMBEDDING_SIZE]), \n",
    "             signs],\n",
    "            axis=2)\n",
    "        \n",
    "        PREEMBEDDING_SIZE = EMBEDDING_SIZE * CLAUSE_SIZE + CLAUSE_SIZE\n",
    "        assert_shape(clause_preembeddings, \n",
    "                     [batch_size, None, PREEMBEDDING_SIZE])\n",
    "        \n",
    "        clause_w = tf.Variable(tf.random_normal(\n",
    "            [PREEMBEDDING_SIZE, EMBEDDING_SIZE]), name='clause_w')\n",
    "        clause_b = tf.Variable(tf.random_normal([EMBEDDING_SIZE]), name='clause_b')\n",
    "        clause_embeddings = tf.reshape(tf.sigmoid(\n",
    "            tf.reshape(clause_preembeddings, [-1, PREEMBEDDING_SIZE]) @ clause_w + clause_b), \n",
    "                                       [batch_size, -1, EMBEDDING_SIZE])\n",
    "        # shape: [None, None, EMBEDDING_SIZE]\n",
    "        \n",
    "        lstm = tf.contrib.rnn.BasicLSTMCell(LSTM_STATE_SIZE)\n",
    "        hidden_state = tf.zeros([batch_size, LSTM_STATE_SIZE])\n",
    "        current_state = tf.zeros([batch_size, LSTM_STATE_SIZE])\n",
    "        state = hidden_state, current_state\n",
    "        \n",
    "        _, lstm_final_state = tf.nn.dynamic_rnn(lstm, clause_embeddings, dtype=tf.float32, \n",
    "                                               sequence_length=self.lengths\n",
    "                                               )\n",
    "        formula_embedding = lstm_final_state.h\n",
    "            \n",
    "        assert_shape(formula_embedding, [batch_size, LSTM_STATE_SIZE])\n",
    "            \n",
    "        softmax_w = tf.Variable(tf.random_normal([LSTM_STATE_SIZE, VARIABLE_NUM*2]), name='softmax_w')\n",
    "        softmax_b = tf.Variable(tf.random_normal([VARIABLE_NUM*2]), name='softmax_b')\n",
    "        \n",
    "        self.policy_logits = formula_embedding @ softmax_w + softmax_b\n",
    "        self.loss = tf.losses.sigmoid_cross_entropy(self.policy_labels, self.policy_logits) \n",
    "        self.policy_probabilities = tf.sigmoid(self.policy_logits)\n",
    "        self.policy_error = tf.reduce_mean(tf.abs(\n",
    "            tf.round(self.policy_probabilities) - self.policy_labels))\n",
    "        \n",
    "        tf.summary.scalar(\"loss\", self.loss)\n",
    "        tf.summary.scalar(\"policy_error\", self.policy_error)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "model = Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=2, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cnf import get_random_kcnfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sat_array():\n",
    "    for k in range(1, 6):\n",
    "        for var_num in range(2, 5):\n",
    "            for clause_num in range(3, 10):\n",
    "                sat = {True: 0, False: 0}\n",
    "                for _ in range(100):\n",
    "                    sat[cnf.get_random_kcnf(k, var_num, clause_num).satisfiable()] += 1\n",
    "                print(k, var_num, clause_num, sat)\n",
    "# sat_array()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"info:\", SAMPLE, CLAUSE_SIZE, VARIABLE_NUM, CLAUSE_NUM)\n",
    "\n",
    "cnfs = get_random_kcnfs(SAMPLE, CLAUSE_SIZE, VARIABLE_NUM, CLAUSE_NUM, min_clause_number=3)\n",
    "plt.title(\"original cnfs: {}\".format(len(cnfs)))\n",
    "plt.hist([len(cnf.clauses) for cnf in cnfs], bins=range(CLAUSE_NUM+2))\n",
    "plt.xlabel(\"num of clauses\")\n",
    "plt.show()\n",
    "cnfs = [cnf for cnf in cnfs if cnf.satisfiable()]\n",
    "plt.title(\"sat cnfs: {}\".format(len(cnfs)))\n",
    "plt.hist([len(cnf.clauses) for cnf in cnfs], bins=range(CLAUSE_NUM+2))\n",
    "plt.xlabel(\"num of clauses\")\n",
    "plt.show()\n",
    "print(\"#samples:\", len(cnfs))\n",
    "assert len(cnfs) * 2 > SAMPLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#labels = [cnf.satisfiable() for cnf in cnfs]\n",
    "labels = []\n",
    "for cnf in cnfs:\n",
    "    correct_steps = cnf.get_correct_steps()\n",
    "    label = []\n",
    "    # for every variable, for true, then for false\n",
    "    for v in range(1, VARIABLE_NUM+1):\n",
    "        for sv in [v, -v]:\n",
    "            result = 1.0 if sv in correct_steps else 0.0\n",
    "            label.append(result)\n",
    "    labels.append(label)\n",
    "assert all(len(label) == 2*VARIABLE_NUM for label in labels)\n",
    "sat_steps = sum(sum(label) for label in labels)\n",
    "unsat_steps = len(cnfs)*2*VARIABLE_NUM - sat_steps\n",
    "print(\"SAT/UNSAT steps: {} / {}\".format(sat_steps, unsat_steps))\n",
    "assert unsat_steps * 4 > sat_steps\n",
    "print(\"labels generated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"num of clauses vs %correct steps\")\n",
    "plt.scatter([len(cnf.clauses) for cnf in cnfs], [sum(label)/2./VARIABLE_NUM for label in labels], alpha=0.5)\n",
    "plt.xlabel(\"num of clauses\")\n",
    "plt.ylabel(\"percent of correct steps\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnfs_train, cnfs_test, labels_train, labels_test = train_test_split(cnfs, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(lists, chunk_size):\n",
    "    return [[it[i:i + chunk_size] for it in lists] for i in range(0, len(lists[0]), chunk_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "merged_summaries = tf.summary.merge_all()\n",
    "\n",
    "SUMMARY_DIR = \"summaries\"\n",
    "MODEL_NAME = \"onlypolicy\"\n",
    "DATESTR = datetime.datetime.now().strftime(\"%y-%m-%d-%H%M%S\")\n",
    "SUMMARY_PREFIX = SUMMARY_DIR + \"/\" + MODEL_NAME + \"-\" + DATESTR\n",
    "train_writer = tf.summary.FileWriter(SUMMARY_PREFIX + \"-train\")\n",
    "test_writer = tf.summary.FileWriter(SUMMARY_PREFIX + \"-test\")\n",
    "\n",
    "NAME = \"{} {}-SATs with {} vars and {} clauses\".format(len(cnfs), CLAUSE_SIZE, VARIABLE_NUM, CLAUSE_NUM)\n",
    "glob_losses = []\n",
    "glob_accs = []\n",
    "glob_test_losses = []\n",
    "glob_test_accs = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate=0.01).minimize(model.loss)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    EPOCHS = 10000\n",
    "    PRINTS = 10\n",
    "    global_step = 0\n",
    "    for epoch_num in range(EPOCHS):\n",
    "        print(\"Epoch\", epoch_num)\n",
    "        losses = []\n",
    "        accs = []\n",
    "        test_losses = []\n",
    "        test_accs = []\n",
    "        cnfs_train, labels_train = shuffle(cnfs_train, labels_train)\n",
    "        for (batch_cnfs, batch_labels) in chunks((cnfs_train, labels_train), batch_size):\n",
    "            if len(batch_cnfs) < batch_size:\n",
    "                continue\n",
    "            inputs, lengths = pad_and_concat([cnf.clauses for cnf in batch_cnfs])\n",
    "            summary, _, loss, probs = sess.run([merged_summaries, train_op, model.loss, model.policy_probabilities], feed_dict={\n",
    "                model.inputs: inputs,\n",
    "                model.policy_labels: batch_labels,\n",
    "                model.lengths: lengths\n",
    "            })\n",
    "            train_writer.add_summary(summary, global_step)\n",
    "            \n",
    "            global_step += 1\n",
    "\n",
    "        \n",
    "        for (batch_cnfs, batch_labels) in chunks((cnfs_test, labels_test), batch_size):\n",
    "            if len(batch_cnfs) < batch_size:\n",
    "                continue\n",
    "            inputs, lengths = pad_and_concat([cnf.clauses for cnf in batch_cnfs])\n",
    "            summary, loss, probs = sess.run([merged_summaries, model.loss, model.policy_probabilities], feed_dict={\n",
    "                model.inputs: inputs,\n",
    "                model.policy_labels: batch_labels,\n",
    "                model.lengths: lengths\n",
    "            })\n",
    "            test_writer.add_summary(summary, global_step)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
