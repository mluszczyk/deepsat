{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_and_concat(sequences):  # sequences shape: [batch_size, len, dims...] -> ([batch_size, maxlen, dims...], [len])\n",
    "    arrays = [np.asarray(seq) for seq in sequences]\n",
    "    lengths = np.asarray([array.shape[0] for array in arrays], dtype=np.int32)\n",
    "    maxlen = np.max(lengths)\n",
    "    arrays = [np.pad(array, [(0, maxlen - array.shape[0]), (0, 0)], 'constant', constant_values=0) for array in arrays]\n",
    "    return np.asarray(arrays), lengths\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_cnf, small_lengths = pad_and_concat(\n",
    "    [[[1, -2], [2, 1]],\n",
    "     [[-2, -1], [1, -2]],\n",
    "     [[-1, -1], [-2, -2], [-1, -2]],\n",
    "      [[1, 1], [-1, -1]],\n",
    "      [[1, -1], [1, 2], [-1, -2]],\n",
    "      [[-2, -2], [2, 2]]\n",
    "     ])\n",
    "small_sol = np.asarray([1, 1, 1, 0, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 1 -2]\n",
      "  [ 2  1]\n",
      "  [ 0  0]]\n",
      "\n",
      " [[-2 -1]\n",
      "  [ 1 -2]\n",
      "  [ 0  0]]\n",
      "\n",
      " [[-1 -1]\n",
      "  [-2 -2]\n",
      "  [-1 -2]]\n",
      "\n",
      " [[ 1  1]\n",
      "  [-1 -1]\n",
      "  [ 0  0]]\n",
      "\n",
      " [[ 1 -1]\n",
      "  [ 1  2]\n",
      "  [-1 -2]]\n",
      "\n",
      " [[-2 -2]\n",
      "  [ 2  2]\n",
      "  [ 0  0]]]\n",
      "[2 2 3 2 3 2]\n"
     ]
    }
   ],
   "source": [
    "print(small_cnf)\n",
    "print(small_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "VARIABLE_NUM = 3\n",
    "EMBEDDING_SIZE = 8\n",
    "CLAUSE_SIZE = 2\n",
    "LSTM_STATE_SIZE = 8\n",
    "batch_size = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assert_shape(matrix, shape: list):\n",
    "    act_shape = matrix.get_shape().as_list()\n",
    "    assert act_shape == shape, \"got shape {}, expected {}\".format(act_shape, shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Graph:\n",
    "    def __init__(self):\n",
    "        self.inputs = tf.placeholder(tf.int32, shape=(batch_size, None, CLAUSE_SIZE), name='inputs')\n",
    "        self.lengths = tf.placeholder(tf.int32, shape=(batch_size,), name='lengths')\n",
    "        self.labels = tf.placeholder(tf.float32, shape=(batch_size,), name='labels')\n",
    "        \n",
    "        vars_ = tf.abs(self.inputs)\n",
    "        signs = tf.cast(tf.sign(self.inputs), tf.float32)  # shape: [batch_size, None, CLAUSE_SIZE]\n",
    "\n",
    "        embeddings = tf.Variable(tf.random_uniform([VARIABLE_NUM + 1, EMBEDDING_SIZE], -1., 1), name='embeddings')\n",
    "\n",
    "        var_embeddings = tf.nn.embedding_lookup(embeddings, vars_)\n",
    "        # var_embeddings shape: [None, None, CLAUSE_SIZE, EMBEDDING_SIZE]\n",
    "        \n",
    "        clause_preembeddings = tf.concat(\n",
    "            [tf.reshape(var_embeddings, [batch_size, -1, CLAUSE_SIZE * EMBEDDING_SIZE]), \n",
    "             signs],\n",
    "            axis=2)\n",
    "        \n",
    "        PREEMBEDDING_SIZE = EMBEDDING_SIZE * CLAUSE_SIZE + CLAUSE_SIZE\n",
    "        assert_shape(clause_preembeddings, \n",
    "                     [batch_size, None, PREEMBEDDING_SIZE])\n",
    "        \n",
    "        clause_w = tf.Variable(tf.random_normal(\n",
    "            [PREEMBEDDING_SIZE, EMBEDDING_SIZE]), name='clause_w')\n",
    "        clause_b = tf.Variable(tf.random_normal([EMBEDDING_SIZE]), name='clause_b')\n",
    "        clause_embeddings = tf.reshape(tf.sigmoid(\n",
    "            tf.reshape(clause_preembeddings, [-1, PREEMBEDDING_SIZE]) @ clause_w + clause_b), \n",
    "                                       [batch_size, -1, EMBEDDING_SIZE])\n",
    "        # shape: [None, None, EMBEDDING_SIZE]\n",
    "        \n",
    "        lstm = tf.contrib.rnn.BasicLSTMCell(LSTM_STATE_SIZE)\n",
    "        hidden_state = tf.zeros([batch_size, LSTM_STATE_SIZE])\n",
    "        current_state = tf.zeros([batch_size, LSTM_STATE_SIZE])\n",
    "        state = hidden_state, current_state\n",
    "        \n",
    "        _, lstm_final_state = tf.nn.dynamic_rnn(lstm, clause_embeddings, dtype=tf.float32, \n",
    "                                               sequence_length=self.lengths\n",
    "                                               )\n",
    "        formula_embedding = lstm_final_state.h\n",
    "            \n",
    "        assert_shape(formula_embedding, [batch_size, LSTM_STATE_SIZE])\n",
    "            \n",
    "        softmax_w = tf.Variable(tf.random_normal([LSTM_STATE_SIZE, 1]), name='softmax_w')\n",
    "        softmax_b = tf.Variable(tf.random_normal([1]), name='softmax_b')\n",
    "        \n",
    "        self.logits = tf.squeeze(formula_embedding @ softmax_w, axis=1) + softmax_b\n",
    "        self.loss = tf.losses.sigmoid_cross_entropy(self.labels, self.logits) \n",
    "        self.probabilities = tf.sigmoid(self.logits)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "model = Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=2, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0.78, 0.77, 0.78, 0.8 , 0.76, 0.79], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    probs = sess.run([model.probabilities], feed_dict={\n",
    "        model.inputs: small_cnf,\n",
    "        model.lengths: small_lengths\n",
    "    })\n",
    "    \n",
    "print(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate=0.1).minimize(model.loss)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for _ in range(100):\n",
    "        sess.run(train_op, feed_dict={\n",
    "            model.inputs: small_cnf,\n",
    "            model.labels: small_sol,\n",
    "            model.lengths: small_lengths\n",
    "        })\n",
    "    probs = sess.run(model.probabilities, feed_dict={\n",
    "        model.inputs: small_cnf,\n",
    "        model.lengths: small_lengths\n",
    "    })\n",
    "    \n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cnf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sat_array():\n",
    "    for k in range(1, 6):\n",
    "        for var_num in range(2, 5):\n",
    "            for clause_num in range(3, 10):\n",
    "                sat = {True: 0, False: 0}\n",
    "                for _ in range(100):\n",
    "                    sat[cnf.get_random_kcnf(k, var_num, clause_num).satisfiable()] += 1\n",
    "                print(k, var_num, clause_num, sat)\n",
    "#sat_array()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "def generate_balanced_set(max_num):\n",
    "    sat = {True: [], False: []}\n",
    "    for formula in cnf.get_random_kcnfs(1000, 2, 3, 10):\n",
    "        sat[formula.satisfiable()].append(formula)\n",
    "        \n",
    "    minlen = min(len(sat[True]), len(sat[False]))\n",
    "    del sat[True][minlen:]\n",
    "    del sat[False][minlen:]\n",
    "    cnfs = sat[True] + sat[False]\n",
    "    labels = [True] * minlen + [False] * minlen\n",
    "    shuffle(cnfs, labels)\n",
    "    return cnfs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_cnfs, test_cnfs, train_labels, test_labels = train_test_split(*generate_balanced_set(1000), test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train sat: 214 train unsat: 214\n",
      "test sat: 24 train unsat: 24\n"
     ]
    }
   ],
   "source": [
    "print(\"train sat:\", sum(1 for label in train_labels if label == True), \"train unsat:\", sum(1 for label in train_labels if label == False))\n",
    "print(\"test sat:\", sum(1 for label in test_labels if label == True), \"train unsat:\", sum(1 for label in test_labels if label == False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(lists, chunk_size):\n",
    "    return [[it[i:i + chunk_size] for it in lists] for i in range(0, len(lists[0]), chunk_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[0, 1], [10, 11]],\n",
       " [[2, 3], [12, 13]],\n",
       " [[4, 5], [14, 15]],\n",
       " [[6, 7], [16, 17]],\n",
       " [[8, 9], [18, 19]]]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks([list(range(10)), list(range(10, 20))], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "skipping incomplete batch\n",
      "train loss: 0.6092559 train acc: 0.685446002953489\n",
      "test loss: 0.5293864 test acc: 0.7291666641831398\n",
      "Epoch 1\n",
      "skipping incomplete batch\n",
      "train loss: 0.5444753 train acc: 0.723004688469457\n",
      "test loss: 0.49674332 test acc: 0.75\n",
      "Epoch 2\n",
      "skipping incomplete batch\n",
      "train loss: 0.5109746 train acc: 0.7535211213038001\n",
      "test loss: 0.4832041 test acc: 0.75\n",
      "Epoch 3\n",
      "skipping incomplete batch\n",
      "train loss: 0.49179855 train acc: 0.7488262873720115\n",
      "test loss: 0.4761938 test acc: 0.729166666045785\n",
      "Epoch 4\n",
      "skipping incomplete batch\n",
      "train loss: 0.4722482 train acc: 0.7793427185273506\n",
      "test loss: 0.4629519 test acc: 0.7499999981373549\n",
      "Epoch 5\n",
      "skipping incomplete batch\n",
      "train loss: 0.46139812 train acc: 0.7723004650062238\n",
      "test loss: 0.45140305 test acc: 0.7499999981373549\n",
      "Epoch 6\n",
      "skipping incomplete batch\n",
      "train loss: 0.46384522 train acc: 0.7699530475156408\n",
      "test loss: 0.4511507 test acc: 0.8124999944120646\n",
      "Epoch 7\n",
      "skipping incomplete batch\n",
      "train loss: 0.4459896 train acc: 0.7793427181075996\n",
      "test loss: 0.44922158 test acc: 0.7916666604578495\n",
      "Epoch 8\n",
      "skipping incomplete batch\n",
      "train loss: 0.4415803 train acc: 0.776995300407141\n",
      "test loss: 0.45243958 test acc: 0.8124999944120646\n",
      "Epoch 9\n",
      "skipping incomplete batch\n",
      "train loss: 0.43304917 train acc: 0.7816901362278093\n",
      "test loss: 0.44855082 test acc: 0.7916666604578495\n",
      "Epoch 10\n",
      "skipping incomplete batch\n",
      "train loss: 0.42191577 train acc: 0.7934272255696041\n",
      "test loss: 0.44220924 test acc: 0.8124999962747097\n",
      "Epoch 11\n",
      "skipping incomplete batch\n",
      "train loss: 0.4124965 train acc: 0.7957746434799382\n",
      "test loss: 0.4370002 test acc: 0.8124999962747097\n",
      "Epoch 12\n",
      "skipping incomplete batch\n",
      "train loss: 0.40250018 train acc: 0.800469479510482\n",
      "test loss: 0.4320429 test acc: 0.8124999962747097\n",
      "Epoch 13\n",
      "skipping incomplete batch\n",
      "train loss: 0.39214012 train acc: 0.8075117330316087\n",
      "test loss: 0.4297382 test acc: 0.8124999962747097\n",
      "Epoch 14\n",
      "skipping incomplete batch\n",
      "train loss: 0.38274467 train acc: 0.8075117326118577\n",
      "test loss: 0.43291163 test acc: 0.8124999962747097\n",
      "Epoch 15\n",
      "skipping incomplete batch\n",
      "train loss: 0.37513807 train acc: 0.8098591505221917\n",
      "test loss: 0.4378076 test acc: 0.7916666623204947\n",
      "Epoch 16\n",
      "skipping incomplete batch\n",
      "train loss: 0.3698108 train acc: 0.8192488221635281\n",
      "test loss: 0.43423986 test acc: 0.7916666623204947\n",
      "Epoch 17\n",
      "skipping incomplete batch\n",
      "train loss: 0.3666153 train acc: 0.8145539861329845\n",
      "test loss: 0.43677732 test acc: 0.8124999962747097\n",
      "Epoch 18\n",
      "skipping incomplete batch\n",
      "train loss: 0.36258003 train acc: 0.8262910754747794\n",
      "test loss: 0.4307778 test acc: 0.8333333302289248\n",
      "Epoch 19\n",
      "skipping incomplete batch\n",
      "train loss: 0.35467407 train acc: 0.8333333292057816\n",
      "test loss: 0.42771322 test acc: 0.8124999962747097\n",
      "Epoch 20\n",
      "skipping incomplete batch\n",
      "train loss: 0.36513957 train acc: 0.8262910752649039\n",
      "test loss: 0.43395096 test acc: 0.8333333283662796\n",
      "Epoch 21\n",
      "skipping incomplete batch\n",
      "train loss: 0.35375786 train acc: 0.8356807464864892\n",
      "test loss: 0.39166573 test acc: 0.7916666623204947\n",
      "Epoch 22\n",
      "skipping incomplete batch\n",
      "train loss: 0.33141154 train acc: 0.8544600895592864\n",
      "test loss: 0.39475608 test acc: 0.8333333283662796\n",
      "Epoch 23\n",
      "skipping incomplete batch\n",
      "train loss: 0.32081598 train acc: 0.8521126724884543\n",
      "test loss: 0.4316826 test acc: 0.8333333283662796\n",
      "Epoch 24\n",
      "skipping incomplete batch\n",
      "train loss: 0.3038838 train acc: 0.8779342688725028\n",
      "test loss: 0.42591396 test acc: 0.8333333283662796\n",
      "Epoch 25\n",
      "skipping incomplete batch\n",
      "train loss: 0.29976162 train acc: 0.8755868507522933\n",
      "test loss: 0.382823 test acc: 0.8541666623204947\n",
      "Epoch 26\n",
      "skipping incomplete batch\n",
      "train loss: 0.29555002 train acc: 0.8685445970212909\n",
      "test loss: 0.37724894 test acc: 0.8333333283662796\n",
      "Epoch 27\n",
      "skipping incomplete batch\n",
      "train loss: 0.31917852 train acc: 0.8521126722785789\n",
      "test loss: 0.4036255 test acc: 0.8333333283662796\n",
      "Epoch 28\n",
      "skipping incomplete batch\n",
      "train loss: 0.28026873 train acc: 0.8732394328419592\n",
      "test loss: 0.3753252 test acc: 0.8333333283662796\n",
      "Epoch 29\n",
      "skipping incomplete batch\n",
      "train loss: 0.25551698 train acc: 0.894366194034966\n",
      "test loss: 0.37380642 test acc: 0.8749999962747097\n",
      "Epoch 30\n",
      "skipping incomplete batch\n",
      "train loss: 0.25464472 train acc: 0.8896713582142978\n",
      "test loss: 0.38975054 test acc: 0.8124999944120646\n",
      "Epoch 31\n",
      "skipping incomplete batch\n",
      "train loss: 0.30582923 train acc: 0.8591549257997056\n",
      "test loss: 0.50435656 test acc: 0.7708333302289248\n",
      "Epoch 32\n",
      "skipping incomplete batch\n",
      "train loss: 0.31767932 train acc: 0.8474178362480351\n",
      "test loss: 0.40078902 test acc: 0.8124999962747097\n",
      "Epoch 33\n",
      "skipping incomplete batch\n",
      "train loss: 0.27107704 train acc: 0.8732394330518346\n",
      "test loss: 0.3978715 test acc: 0.8541666623204947\n",
      "Epoch 34\n",
      "skipping incomplete batch\n",
      "train loss: 0.23419595 train acc: 0.91079811961718\n",
      "test loss: 0.38886067 test acc: 0.7916666623204947\n",
      "Epoch 35\n",
      "skipping incomplete batch\n",
      "train loss: 0.23988888 train acc: 0.8873239405138392\n",
      "test loss: 0.4479225 test acc: 0.7916666623204947\n",
      "Epoch 36\n",
      "skipping incomplete batch\n",
      "train loss: 0.20087358 train acc: 0.9201877912585165\n",
      "test loss: 0.42533964 test acc: 0.8541666623204947\n",
      "Epoch 37\n",
      "skipping incomplete batch\n",
      "train loss: 0.18542658 train acc: 0.9201877910486409\n",
      "test loss: 0.4977721 test acc: 0.8124999944120646\n",
      "Epoch 38\n",
      "skipping incomplete batch\n",
      "train loss: 0.2481305 train acc: 0.8943661942448414\n",
      "test loss: 0.50650203 test acc: 0.8124999944120646\n",
      "Epoch 39\n",
      "skipping incomplete batch\n",
      "train loss: 0.2042422 train acc: 0.906103283796512\n",
      "test loss: 0.6259057 test acc: 0.7499999962747097\n",
      "Epoch 40\n",
      "skipping incomplete batch\n",
      "train loss: 0.19252412 train acc: 0.9248826270791847\n",
      "test loss: 0.48534763 test acc: 0.8124999962747097\n",
      "Epoch 41\n",
      "skipping incomplete batch\n",
      "train loss: 0.18293247 train acc: 0.934272298720521\n",
      "test loss: 0.67082614 test acc: 0.7291666623204947\n",
      "Epoch 42\n",
      "skipping incomplete batch\n",
      "train loss: 0.1501971 train acc: 0.9436619701519818\n",
      "test loss: 0.6787686 test acc: 0.7708333320915699\n",
      "Epoch 43\n",
      "skipping incomplete batch\n",
      "train loss: 0.13649157 train acc: 0.9600938955243205\n",
      "test loss: 0.7320621 test acc: 0.7291666623204947\n",
      "Epoch 44\n",
      "skipping incomplete batch\n",
      "train loss: 0.12439493 train acc: 0.9624413134346546\n",
      "test loss: 0.7517632 test acc: 0.7083333246409893\n",
      "Epoch 45\n",
      "skipping incomplete batch\n",
      "train loss: 0.111674495 train acc: 0.9671361492553228\n",
      "test loss: 0.7466403 test acc: 0.7708333302289248\n",
      "Epoch 46\n",
      "skipping incomplete batch\n",
      "train loss: 0.11373483 train acc: 0.9624413134346546\n",
      "test loss: 0.7363893 test acc: 0.7708333265036345\n",
      "Epoch 47\n",
      "skipping incomplete batch\n",
      "train loss: 0.22906417 train acc: 0.9295774626899773\n",
      "test loss: 0.6565049 test acc: 0.7291666641831398\n",
      "Epoch 48\n",
      "skipping incomplete batch\n",
      "train loss: 0.23080267 train acc: 0.8967136121551755\n",
      "test loss: 0.5648396 test acc: 0.7291666623204947\n",
      "Epoch 49\n",
      "skipping incomplete batch\n",
      "train loss: 0.17174086 train acc: 0.9342722985106455\n",
      "test loss: 0.58259827 test acc: 0.7708333320915699\n",
      "Epoch 50\n",
      "skipping incomplete batch\n",
      "train loss: 0.17671864 train acc: 0.9366197166308551\n",
      "test loss: 0.5990479 test acc: 0.7083333283662796\n",
      "Epoch 51\n",
      "skipping incomplete batch\n",
      "train loss: 0.1441888 train acc: 0.9507042238829841\n",
      "test loss: 0.4816835 test acc: 0.7291666623204947\n",
      "Epoch 52\n",
      "skipping incomplete batch\n",
      "train loss: 0.14194454 train acc: 0.9530516417933182\n",
      "test loss: 0.64535576 test acc: 0.7291666623204947\n",
      "Epoch 53\n",
      "skipping incomplete batch\n",
      "train loss: 0.10482866 train acc: 0.9647887313449887\n",
      "test loss: 0.69367903 test acc: 0.7499999944120646\n",
      "Epoch 54\n",
      "skipping incomplete batch\n",
      "train loss: 0.09107847 train acc: 0.9647887313449887\n",
      "test loss: 0.8955007 test acc: 0.7916666623204947\n",
      "Epoch 55\n",
      "skipping incomplete batch\n",
      "train loss: 0.10101834 train acc: 0.9647887313449887\n",
      "test loss: 0.8576032 test acc: 0.6874999962747097\n",
      "Epoch 56\n",
      "skipping incomplete batch\n",
      "train loss: 0.09530446 train acc: 0.971830985075991\n",
      "test loss: 0.70058537 test acc: 0.7499999981373549\n",
      "Epoch 57\n",
      "skipping incomplete batch\n",
      "train loss: 0.21550028 train acc: 0.9342722985106455\n",
      "test loss: 0.49497855 test acc: 0.8333333283662796\n",
      "Epoch 58\n",
      "skipping incomplete batch\n",
      "train loss: 0.13981275 train acc: 0.9530516417933182\n",
      "test loss: 0.59034735 test acc: 0.7499999981373549\n",
      "Epoch 59\n",
      "skipping incomplete batch\n",
      "train loss: 0.16655779 train acc: 0.9507042238829841\n",
      "test loss: 0.67065537 test acc: 0.7708333302289248\n",
      "Epoch 60\n",
      "skipping incomplete batch\n",
      "train loss: 0.13890027 train acc: 0.946009388482067\n",
      "test loss: 0.86316764 test acc: 0.7291666623204947\n",
      "Epoch 61\n",
      "skipping incomplete batch\n",
      "train loss: 0.11834154 train acc: 0.9577464778238619\n",
      "test loss: 0.73718524 test acc: 0.7499999981373549\n",
      "Epoch 62\n",
      "skipping incomplete batch\n",
      "train loss: 0.07426521 train acc: 0.9835680746276614\n",
      "test loss: 0.71744525 test acc: 0.7291666623204947\n",
      "Epoch 63\n",
      "skipping incomplete batch\n",
      "train loss: 0.061194856 train acc: 0.9906103283586637\n",
      "test loss: 0.8212449 test acc: 0.7083333283662796\n",
      "Epoch 64\n",
      "skipping incomplete batch\n",
      "train loss: 0.05547113 train acc: 0.9906103283586637\n",
      "test loss: 0.8335697 test acc: 0.7083333283662796\n",
      "Epoch 65\n",
      "skipping incomplete batch\n",
      "train loss: 0.05094176 train acc: 0.9906103283586637\n",
      "test loss: 0.8180557 test acc: 0.7083333283662796\n",
      "Epoch 66\n",
      "skipping incomplete batch\n",
      "train loss: 0.048195653 train acc: 0.9929577462689977\n",
      "test loss: 0.84061074 test acc: 0.6874999925494194\n",
      "Epoch 67\n",
      "skipping incomplete batch\n",
      "train loss: 0.044631496 train acc: 0.9929577462689977\n",
      "test loss: 0.7999382 test acc: 0.6874999925494194\n",
      "Epoch 68\n",
      "skipping incomplete batch\n",
      "train loss: 0.04179477 train acc: 0.9929577462689977\n",
      "test loss: 0.7437237 test acc: 0.6874999925494194\n",
      "Epoch 69\n",
      "skipping incomplete batch\n",
      "train loss: 0.03909575 train acc: 0.9929577462689977\n",
      "test loss: 0.6718925 test acc: 0.7083333283662796\n",
      "Epoch 70\n",
      "skipping incomplete batch\n",
      "train loss: 0.03709429 train acc: 0.9929577462689977\n",
      "test loss: 0.47735453 test acc: 0.8124999981373549\n",
      "Epoch 71\n",
      "skipping incomplete batch\n",
      "train loss: 0.10991389 train acc: 0.9647887315548641\n",
      "test loss: 0.54122615 test acc: 0.8124999962747097\n",
      "Epoch 72\n",
      "skipping incomplete batch\n",
      "train loss: 0.20508014 train acc: 0.9107981198270556\n",
      "test loss: 0.52598727 test acc: 0.8124999981373549\n",
      "Epoch 73\n",
      "skipping incomplete batch\n",
      "train loss: 0.12053277 train acc: 0.9577464776139863\n",
      "test loss: 0.5047709 test acc: 0.8124999962747097\n",
      "Epoch 74\n",
      "skipping incomplete batch\n",
      "train loss: 0.08808296 train acc: 0.9624413134346546\n",
      "test loss: 0.57284725 test acc: 0.7291666623204947\n",
      "Epoch 75\n",
      "skipping incomplete batch\n",
      "train loss: 0.055296946 train acc: 0.9812206567173273\n",
      "test loss: 0.6159947 test acc: 0.7708333302289248\n",
      "Epoch 76\n",
      "skipping incomplete batch\n",
      "train loss: 0.044430036 train acc: 0.9906103283586637\n",
      "test loss: 0.6432865 test acc: 0.7499999981373549\n",
      "Epoch 77\n",
      "skipping incomplete batch\n",
      "train loss: 0.038278036 train acc: 0.9929577462689977\n",
      "test loss: 0.6492545 test acc: 0.7499999981373549\n",
      "Epoch 78\n",
      "skipping incomplete batch\n",
      "train loss: 0.032745026 train acc: 0.9929577462689977\n",
      "test loss: 0.66992915 test acc: 0.7708333320915699\n",
      "Epoch 79\n",
      "skipping incomplete batch\n",
      "train loss: 0.02917998 train acc: 0.9953051641793318\n",
      "test loss: 0.68834853 test acc: 0.7708333320915699\n",
      "Epoch 80\n",
      "skipping incomplete batch\n",
      "train loss: 0.025911283 train acc: 0.9953051641793318\n",
      "test loss: 0.7050241 test acc: 0.7708333320915699\n",
      "Epoch 81\n",
      "skipping incomplete batch\n",
      "train loss: 0.023114502 train acc: 0.9953051641793318\n",
      "test loss: 0.7256638 test acc: 0.7499999962747097\n",
      "Epoch 82\n",
      "skipping incomplete batch\n",
      "train loss: 0.020620186 train acc: 0.9976525820896659\n",
      "test loss: 0.75460494 test acc: 0.7291666623204947\n",
      "Epoch 83\n",
      "skipping incomplete batch\n",
      "train loss: 0.018428862 train acc: 0.9976525820896659\n",
      "test loss: 0.78913474 test acc: 0.7499999981373549\n",
      "Epoch 84\n",
      "skipping incomplete batch\n",
      "train loss: 0.016538657 train acc: 0.9976525820896659\n",
      "test loss: 0.8220847 test acc: 0.7291666641831398\n",
      "Epoch 85\n",
      "skipping incomplete batch\n",
      "train loss: 0.0149081405 train acc: 0.9976525820896659\n",
      "test loss: 0.85026175 test acc: 0.7291666641831398\n",
      "Epoch 86\n",
      "skipping incomplete batch\n",
      "train loss: 0.013494152 train acc: 1.0\n",
      "test loss: 0.8782779 test acc: 0.7291666641831398\n",
      "Epoch 87\n",
      "skipping incomplete batch\n",
      "train loss: 0.012299707 train acc: 1.0\n",
      "test loss: 0.9067807 test acc: 0.7291666641831398\n",
      "Epoch 88\n",
      "skipping incomplete batch\n",
      "train loss: 0.011283034 train acc: 1.0\n",
      "test loss: 0.9328321 test acc: 0.7083333302289248\n",
      "Epoch 89\n",
      "skipping incomplete batch\n",
      "train loss: 0.010394096 train acc: 1.0\n",
      "test loss: 0.95512104 test acc: 0.7083333302289248\n",
      "Epoch 90\n",
      "skipping incomplete batch\n",
      "train loss: 0.009606315 train acc: 1.0\n",
      "test loss: 0.9741719 test acc: 0.7083333302289248\n",
      "Epoch 91\n",
      "skipping incomplete batch\n",
      "train loss: 0.0089052655 train acc: 1.0\n",
      "test loss: 0.9906783 test acc: 0.7083333302289248\n",
      "Epoch 92\n",
      "skipping incomplete batch\n",
      "train loss: 0.008279183 train acc: 1.0\n",
      "test loss: 1.0050719 test acc: 0.7083333302289248\n",
      "Epoch 93\n",
      "skipping incomplete batch\n",
      "train loss: 0.007717735 train acc: 1.0\n",
      "test loss: 1.0176638 test acc: 0.7083333302289248\n",
      "Epoch 94\n",
      "skipping incomplete batch\n",
      "train loss: 0.0072117615 train acc: 1.0\n",
      "test loss: 1.0287141 test acc: 0.7083333302289248\n",
      "Epoch 95\n",
      "skipping incomplete batch\n",
      "train loss: 0.0067531574 train acc: 1.0\n",
      "test loss: 1.0384431 test acc: 0.7083333302289248\n",
      "Epoch 96\n",
      "skipping incomplete batch\n",
      "train loss: 0.006335112 train acc: 1.0\n",
      "test loss: 1.0470524 test acc: 0.7083333302289248\n",
      "Epoch 97\n",
      "skipping incomplete batch\n",
      "train loss: 0.005952137 train acc: 1.0\n",
      "test loss: 1.0547438 test acc: 0.7083333302289248\n",
      "Epoch 98\n",
      "skipping incomplete batch\n",
      "train loss: 0.005599829 train acc: 1.0\n",
      "test loss: 1.0617261 test acc: 0.7083333302289248\n",
      "Epoch 99\n",
      "skipping incomplete batch\n",
      "train loss: 0.0052746567 train acc: 1.0\n",
      "test loss: 1.0682003 test acc: 0.7083333302289248\n",
      "first batch:\n",
      "actual: [0.   0.76 0.01 1.   0.27 1.  ]\n",
      "expected [True, False, True, True, True, False]\n",
      "new data:\n",
      "actual [1. 1. 1. 1. 1. 1.]\n",
      "expected [1 1 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate=0.01).minimize(model.loss)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for epoch_num in range(100):\n",
    "        print(\"Epoch\", epoch_num)\n",
    "        losses = []\n",
    "        accs = []\n",
    "        for (batch_cnfs, batch_labels) in chunks((train_cnfs, train_labels), batch_size):\n",
    "            if len(batch_cnfs) < batch_size:\n",
    "                print(\"skipping incomplete batch\")\n",
    "                continue\n",
    "            inputs, lengths = pad_and_concat([cnf.clauses for cnf in batch_cnfs])\n",
    "            _, loss, probs = sess.run([train_op, model.loss, model.probabilities], feed_dict={\n",
    "                model.inputs: inputs,\n",
    "                model.labels: batch_labels,\n",
    "                model.lengths: lengths\n",
    "            })\n",
    "            losses.append(loss)\n",
    "            accs.append(1 - np.mean(np.abs(np.around(probs) - np.asarray(batch_labels))))\n",
    "            \n",
    "        print(\"train loss:\", np.mean(np.asarray(losses)), \"train acc:\", np.mean(accs))\n",
    "        \n",
    "        losses = []\n",
    "        accs = []\n",
    "        for (batch_cnfs, batch_labels) in chunks((test_cnfs, test_labels), batch_size):\n",
    "            if len(batch_cnfs) < batch_size:\n",
    "                print(\"skipping incomplete batch\")\n",
    "                continue\n",
    "            inputs, lengths = pad_and_concat([cnf.clauses for cnf in batch_cnfs])\n",
    "            loss, probs = sess.run([model.loss, model.probabilities], feed_dict={\n",
    "                model.inputs: inputs,\n",
    "                model.lengths: lengths,\n",
    "                model.labels: batch_labels\n",
    "            })\n",
    "            losses.append(loss)\n",
    "            accs.append(1 - np.mean(np.abs(np.around(probs) - np.asarray(batch_labels))))\n",
    "        print('test loss:', np.mean(np.asarray(losses)), 'test acc:', np.mean(accs))\n",
    "\n",
    "\n",
    "    print(\"first batch:\")\n",
    "    first_inputs, first_lengths = pad_and_concat([cnf.clauses for cnf in cnfs[:batch_size]])\n",
    "    probs = sess.run(model.probabilities, feed_dict={\n",
    "        model.inputs: first_inputs,\n",
    "        model.lengths: first_lengths\n",
    "    })\n",
    "    print(\"actual:\", probs)\n",
    "    print(\"expected\", labels[:batch_size])\n",
    "        \n",
    "    print(\"new data:\")\n",
    "    probs = sess.run(model.probabilities, feed_dict={\n",
    "        model.inputs: small_cnf,\n",
    "        model.lengths: small_lengths\n",
    "    })\n",
    "    print(\"actual\", probs)\n",
    "    print(\"expected\", small_sol)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
