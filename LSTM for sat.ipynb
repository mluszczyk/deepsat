{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_and_concat(sequences):  # sequences shape: [batch_size, len, dims...] -> ([batch_size, maxlen, dims...], [len])\n",
    "    arrays = [np.asarray(seq) for seq in sequences]\n",
    "    lengths = np.asarray([array.shape[0] for array in arrays], dtype=np.int32)\n",
    "    maxlen = np.max(lengths)\n",
    "    arrays = [np.pad(array, [(0, maxlen - array.shape[0]), (0, 0)], 'constant', constant_values=0) for array in arrays]\n",
    "    return np.asarray(arrays), lengths\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf, lengths = pad_and_concat(\n",
    "    [[[1, -2], [2, 1]],\n",
    "     [[-2, -1], [1, -2]],\n",
    "     [[-1, -1], [-2, -2], [-1, -2]],\n",
    "      [[1, -1]],\n",
    "      [[1, -1], [1, 2]],\n",
    "      [[-2, 2]]\n",
    "     ])\n",
    "sol = np.asarray([1, 1, 1, 0, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 1 -2]\n",
      "  [ 2  1]\n",
      "  [ 0  0]]\n",
      "\n",
      " [[-2 -1]\n",
      "  [ 1 -2]\n",
      "  [ 0  0]]\n",
      "\n",
      " [[-1 -1]\n",
      "  [-2 -2]\n",
      "  [-1 -2]]\n",
      "\n",
      " [[ 1 -1]\n",
      "  [ 0  0]\n",
      "  [ 0  0]]\n",
      "\n",
      " [[ 1 -1]\n",
      "  [ 1  2]\n",
      "  [ 0  0]]\n",
      "\n",
      " [[-2  2]\n",
      "  [ 0  0]\n",
      "  [ 0  0]]]\n",
      "[2 2 3 1 2 1]\n"
     ]
    }
   ],
   "source": [
    "print(cnf)\n",
    "print(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "VARIABLE_NUM = 2\n",
    "EMBEDDING_SIZE = 8\n",
    "CLAUSE_SIZE = 2\n",
    "LSTM_STATE_SIZE = 8\n",
    "batch_size = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assert_shape(matrix, shape: list):\n",
    "    act_shape = matrix.get_shape().as_list()\n",
    "    assert act_shape == shape, \"got shape {}, expected {}\".format(act_shape, shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Graph:\n",
    "    def __init__(self):\n",
    "        self.inputs = tf.placeholder(tf.int32, shape=(batch_size, None, CLAUSE_SIZE), name='inputs')\n",
    "        self.lengths = tf.placeholder(tf.int32, shape=(batch_size,), name='lengths')\n",
    "        self.labels = tf.placeholder(tf.float32, shape=(batch_size), name='labels')\n",
    "        \n",
    "        vars_ = tf.abs(self.inputs)\n",
    "        signs = tf.cast(tf.sign(self.inputs), tf.float32)  # shape: [batch_size, None, CLAUSE_SIZE]\n",
    "\n",
    "        embeddings = tf.Variable(tf.random_uniform([VARIABLE_NUM + 1, EMBEDDING_SIZE], -1., 1), name='embeddings')\n",
    "\n",
    "        var_embeddings = tf.nn.embedding_lookup(embeddings, vars_)\n",
    "        # var_embeddings shape: [batch_size, None, CLAUSE_SIZE, EMBEDDING_SIZE]\n",
    "        \n",
    "        clause_preembeddings = tf.concat(\n",
    "            [tf.reshape(var_embeddings, [batch_size, -1, CLAUSE_SIZE * EMBEDDING_SIZE]), \n",
    "             signs],\n",
    "            axis=2)\n",
    "        \n",
    "        PREEMBEDDING_SIZE = EMBEDDING_SIZE * CLAUSE_SIZE + CLAUSE_SIZE\n",
    "        assert_shape(clause_preembeddings, \n",
    "                     [batch_size, None, PREEMBEDDING_SIZE])\n",
    "        \n",
    "        clause_w = tf.Variable(tf.random_normal(\n",
    "            [PREEMBEDDING_SIZE, EMBEDDING_SIZE]), name='clause_w')\n",
    "        clause_b = tf.Variable(tf.random_normal([EMBEDDING_SIZE]), name='clause_b')\n",
    "        clause_embeddings = tf.reshape(tf.sigmoid(\n",
    "            tf.reshape(clause_preembeddings, [-1, PREEMBEDDING_SIZE]) @ clause_w + clause_b), \n",
    "                                       [batch_size, -1, EMBEDDING_SIZE])\n",
    "        # shape: [batch_size, None, EMBEDDING_SIZE]\n",
    "        \n",
    "        lstm = tf.contrib.rnn.BasicLSTMCell(LSTM_STATE_SIZE)\n",
    "        hidden_state = tf.zeros([batch_size, LSTM_STATE_SIZE])\n",
    "        current_state = tf.zeros([batch_size, LSTM_STATE_SIZE])\n",
    "        state = hidden_state, current_state\n",
    "        \n",
    "        _, lstm_final_state = tf.nn.dynamic_rnn(lstm, clause_embeddings, dtype=tf.float32, \n",
    "                                               sequence_length=self.lengths\n",
    "                                               )\n",
    "        formula_embedding = lstm_final_state.h\n",
    "            \n",
    "        assert_shape(formula_embedding, [batch_size, LSTM_STATE_SIZE])\n",
    "            \n",
    "        softmax_w = tf.Variable(tf.random_normal([LSTM_STATE_SIZE, 1]), name='softmax_w')\n",
    "        softmax_b = tf.Variable(tf.random_normal([1]), name='softmax_b')\n",
    "        \n",
    "        self.logits = tf.squeeze(formula_embedding @ softmax_w, axis=1) + softmax_b\n",
    "        self.loss = tf.losses.sigmoid_cross_entropy(self.labels, self.logits) \n",
    "        self.probabilities = tf.sigmoid(self.logits)\n",
    "\n",
    "model = Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=2, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0.65, 0.7 , 0.38, 0.96, 0.8 , 0.47], dtype=float32), array([0.66, 0.67, 0.59, 0.72, 0.69, 0.61], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    probs = sess.run([model.logits, model.probabilities], feed_dict={\n",
    "        model.inputs: cnf,\n",
    "        model.lengths: lengths\n",
    "    })\n",
    "    \n",
    "print(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate=0.1).minimize(model.loss)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for _ in range(100):\n",
    "        sess.run(train_op, feed_dict={\n",
    "            model.inputs: cnf,\n",
    "            model.labels: sol,\n",
    "            model.lengths: lengths\n",
    "        })\n",
    "    probs = sess.run(model.probabilities, feed_dict={\n",
    "        model.inputs: cnf,\n",
    "        model.lengths: lengths\n",
    "    })\n",
    "    \n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
